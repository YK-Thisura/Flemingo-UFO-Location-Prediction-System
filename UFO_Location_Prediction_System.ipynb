{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 01. Install Required Packages"
      ],
      "metadata": {
        "id": "bo1zzjq86QMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas numpy scikit-learn nltk joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import nltk\n",
        "import joblib\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.multiclass import unique_labels"
      ],
      "metadata": {
        "id": "KsYHAlUH6Y73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02. Download NLTK Data"
      ],
      "metadata": {
        "id": "eKh_joL36dY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "7vHsWOrT6gC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03. Extract Zip File"
      ],
      "metadata": {
        "id": "GP0fub5E6iVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/archive.zip'\n",
        "extract_to = '/content/data'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "extracted_files = os.listdir(extract_to)\n",
        "print(\"Extracted files and folders:\", extracted_files)\n",
        "\n",
        "data_path = os.path.join(extract_to, 'data')\n",
        "if os.path.exists(data_path):\n",
        "    print(\"Contents of 'data' folder:\", os.listdir(data_path))\n",
        "else:\n",
        "    print(\"'data' folder not found inside the archive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On1RzOot6k4_",
        "outputId": "049ea666-8120-4105-f686-399310c9cbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files and folders: ['complete.csv', 'scrubbed.csv']\n",
            "'data' folder not found inside the archive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04. Define State Abbreviations"
      ],
      "metadata": {
        "id": "42YHHZFw6v8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_FULL_NAMES = {\n",
        "    'al': 'Alabama', 'ak': 'Alaska', 'az': 'Arizona', 'ar': 'Arkansas', 'ca': 'California',\n",
        "    'co': 'Colorado', 'ct': 'Connecticut', 'de': 'Delaware', 'fl': 'Florida', 'ga': 'Georgia',\n",
        "    'hi': 'Hawaii', 'id': 'Idaho', 'il': 'Illinois', 'in': 'Indiana', 'ia': 'Iowa', 'ks': 'Kansas',\n",
        "    'ky': 'Kentucky', 'la': 'Louisiana', 'me': 'Maine', 'md': 'Maryland', 'ma': 'Massachusetts',\n",
        "    'mi': 'Michigan', 'mn': 'Minnesota', 'ms': 'Mississippi', 'mo': 'Missouri', 'mt': 'Montana',\n",
        "    'ne': 'Nebraska', 'nv': 'Nevada', 'nh': 'New Hampshire', 'nj': 'New Jersey', 'nm': 'New Mexico',\n",
        "    'ny': 'New York', 'nc': 'North Carolina', 'nd': 'North Dakota', 'oh': 'Ohio', 'ok': 'Oklahoma',\n",
        "    'or': 'Oregon', 'pa': 'Pennsylvania', 'ri': 'Rhode Island', 'sc': 'South Carolina',\n",
        "    'sd': 'South Dakota', 'tn': 'Tennessee', 'tx': 'Texas', 'ut': 'Utah', 'vt': 'Vermont',\n",
        "    'va': 'Virginia', 'wa': 'Washington', 'wv': 'West Virginia', 'wi': 'Wisconsin', 'wy': 'Wyoming',\n",
        "    'dc': 'District of Columbia', 'ab': 'Alberta', 'bc': 'British Columbia', 'mb': 'Manitoba',\n",
        "    'nb': 'New Brunswick', 'nl': 'Newfoundland and Labrador', 'ns': 'Nova Scotia',\n",
        "    'nt': 'Northwest Territories', 'nu': 'Nunavut', 'on': 'Ontario', 'pe': 'Prince Edward Island',\n",
        "    'qc': 'Quebec', 'sk': 'Saskatchewan', 'yt': 'Yukon', 'nf': 'Newfoundland and Labrador',\n",
        "    'pq': 'Quebec'\n",
        "}"
      ],
      "metadata": {
        "id": "5BwKSQzT6ycO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05.  Define UFO Sighting Predictor Class"
      ],
      "metadata": {
        "id": "IxrXw1Dw60lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UFOSightingPredictor:\n",
        "\n",
        "    def __init__(self, model_path=\"ufo_sighting_model.joblib\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.tfidf = None\n",
        "        self.label_encoder = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A).lower()\n",
        "        tokens = text.split()\n",
        "        tokens = [word for word in tokens if word not in STOPWORDS]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def _load_and_preprocess_data(self, zip_path=\"archive.zip\", data_file=\"data/scrubbed.csv\"):\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        df = pd.read_csv(data_file, low_memory=False)\n",
        "        df = df.dropna(subset=['datetime', 'state', 'duration (seconds)', 'comments'])\n",
        "\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "        df['duration (seconds)'] = pd.to_numeric(df['duration (seconds)'], errors='coerce')\n",
        "        df = df.dropna(subset=['datetime', 'duration (seconds)'])\n",
        "\n",
        "        df['comments'] = df['comments'].apply(self._clean_text)\n",
        "        df['year'] = df['datetime'].dt.year\n",
        "        df['month'] = df['datetime'].dt.month\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['weekday'] = df['datetime'].dt.weekday\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        df['state_label'] = self.label_encoder.fit_transform(df['state'])\n",
        "\n",
        "        self.tfidf = TfidfVectorizer(max_features=500)\n",
        "        X_text = self.tfidf.fit_transform(df['comments']).toarray()\n",
        "        X_time = df[['year', 'month', 'hour', 'weekday', 'duration (seconds)']].values\n",
        "        X = np.concatenate([X_time, X_text], axis=1)\n",
        "        y = df['state_label']\n",
        "        return X, y\n",
        "\n",
        "    def train(self):\n",
        "        X, y = self._load_and_preprocess_data()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(\"Training RandomForest model...\")\n",
        "        self.model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        print(\"\\n=== Model Evaluation ===\")\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        used_labels = unique_labels(y_test, y_pred)\n",
        "        used_target_names = self.label_encoder.inverse_transform(used_labels)\n",
        "        print(classification_report(y_test, y_pred, labels=used_labels, target_names=used_target_names))\n",
        "\n",
        "        self._save_model()\n",
        "\n",
        "    def _save_model(self):\n",
        "        print(f\"Saving model to {self.model_path}...\")\n",
        "        payload = {'model': self.model, 'tfidf': self.tfidf, 'label_encoder': self.label_encoder}\n",
        "        joblib.dump(payload, self.model_path)\n",
        "        print(\"Model saved successfully.\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        if os.path.exists(self.model_path):\n",
        "            print(f\"Loading pre-trained model from {self.model_path}...\")\n",
        "            payload = joblib.load(self.model_path)\n",
        "            self.model = payload['model']\n",
        "            self.tfidf = payload['tfidf']\n",
        "            self.label_encoder = payload['label_encoder']\n",
        "            print(\"Model loaded successfully.\")\n",
        "        else:\n",
        "            print(\"No pre-trained model found.\")\n",
        "\n",
        "    def predict(self, description, sighting_time, duration_sec):\n",
        "        if not all([self.model, self.tfidf, self.label_encoder]):\n",
        "            print(\"‚ùå Error: Model is not trained or loaded.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            dt = datetime.strptime(sighting_time, \"%Y-%m-%d %H:%M\")\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Invalid datetime format. Please use 'YYYY-MM-DD HH:MM'.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            duration_sec = float(duration_sec)\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Invalid duration. Please enter a number for seconds.\")\n",
        "            return\n",
        "\n",
        "        year, month, hour, weekday = dt.year, dt.month, dt.hour, dt.weekday()\n",
        "        clean_text = self._clean_text(description)\n",
        "        text_vec = self.tfidf.transform([clean_text]).toarray()\n",
        "        time_features = np.array([year, month, hour, weekday, duration_sec]).reshape(1, -1)\n",
        "        input_vec = np.concatenate([time_features, text_vec], axis=1)\n",
        "\n",
        "        pred_label = self.model.predict(input_vec)\n",
        "        pred_abbr = self.label_encoder.inverse_transform(pred_label)[0]\n",
        "        pred_full_name = STATE_FULL_NAMES.get(pred_abbr.lower(), pred_abbr.upper())\n",
        "\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(f\"üîÆ Predicted Location: {pred_full_name}\")\n",
        "        print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "    def start_interactive_mode(self):\n",
        "        if not self.model:\n",
        "            print(\"Please train the model first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== üõ∏ Live UFO Sighting Prediction ===\")\n",
        "        print(\"Enter sighting details to predict the location. Type 'quit' at any time to exit.\")\n",
        "\n",
        "        while True:\n",
        "            description = input(\"‚û°Ô∏è Enter sighting description: \")\n",
        "            if description.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            sighting_time = input(\"‚û°Ô∏è Enter sighting time (YYYY-MM-DD HH:MM): \")\n",
        "            if sighting_time.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            duration_sec = input(\"‚û°Ô∏è Enter duration in seconds (e.g., 180): \")\n",
        "            if duration_sec.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            self.predict(description, sighting_time, duration_sec)\n"
      ],
      "metadata": {
        "id": "KcbI5Oq964uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06. Run the Predictor"
      ],
      "metadata": {
        "id": "Xmtb2ldo7Gcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    predictor = UFOSightingPredictor(model_path=\"ufo_sighting_model.joblib\")\n",
        "    if not predictor.model:\n",
        "        print(\"Training a new model as no pre-trained model was found.\")\n",
        "        predictor.train()\n",
        "    predictor.start_interactive_mode()"
      ],
      "metadata": {
        "id": "jg6DgyKq-Lpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}